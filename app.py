# -*- coding: utf-8 -*-
"""Yet another copy of Untitled45.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8u_m2LFtLL_adycTmCcV80EQodmdRZL
"""

from newsapi import NewsApiClient

# Initialize the NewsAPI client with your API key
newsapi = NewsApiClient(api_key='f68a182c98194641883a76a696d35824')

# Fetch top headlines
headlines = newsapi.get_top_headlines(language='en')

# Extract the news
for article in headlines['articles']:
    print(article['title'], article['description'])

from unstructured.partition.auto import partition

# Specify the path to your PDF
file_path = "/content/NetSol_Financial Statement_2024_Part 1 (1).pdf"

# Process the PDF
elements = partition(filename=file_path)

# Inspect the extracted elements
for elem in elements:
    print(elem)

def classify_query(query):
    finance_keywords = ['finance', 'report', 'statement', 'quarterly', 'income']
    event_keywords = ['news', 'current', 'live', 'update']
    general_keywords = ['who', 'what', 'how', 'why']

    if any(word in query.lower() for word in finance_keywords):
        return 'finance'
    elif any(word in query.lower() for word in event_keywords):
        return 'event'
    else:
        return 'general'

query = "What are the latest news updates?"
category = classify_query(query)
print(f"Category: {category}")

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
data = "some_value"  # Define the variable
print(data)  # Use it


# Assuming 'data' is a list of text extracted from the PDF
embeddings = model.encode(data)

import pinecone
from pinecone import Pinecone
from pinecone import Pinecone, ServerlessSpec

from sentence_transformers import SentenceTransformer

# Initialize the embedding model (you can use other models too)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Example text (this could be your PDF data or user query)
text = ["This is a sample text to get embeddings."]

# Generate embeddings
embeddings = embedding_model.encode(text)

# Print the generated embeddings
print(embeddings)



# Initialize Pinecone with your API key
pc = Pinecone(api_key='pcsk_61DPge_71iXudsjFTJh3D9ZFx4KhHEKdZWadpWTipG8X7g7w8eBehXDUJ9u8px1nZkCJkP', environment='us-west1-gcp') # Create a Pinecone instance

# Define index name
index_name = 'financial-report'

# Check if the index exists, if not create it
if index_name not in pc.list_indexes().names():
    # Define the dimension of the embeddings - this should match the dimension of your embeddings
    dimension = embeddings.shape[0]
    # Define the index metric
    metric = 'cosine'  # You can change this to 'euclidean', 'dotproduct', etc.
      # Define the index spec (using ServerlessSpec as an example)
    spec = ServerlessSpec(cloud="aws", region="us-east-1")  # Replace with your desired configuration
    # Create the index
    pc.create_index(name=index_name, dimension=dimension, metric=metric, spec=spec)

# Now access the index (it will be created or exist already)
index = pc.Index(index_name)

from pinecone import PineconeApiException

# Initialize Pinecone
pc = Pinecone(api_key="pcsk_61DPge_71iXudsjFTJh3D9ZFx4KhHEKdZWadpWTipG8X7g7w8eBehXDUJ9u8px1nZkCJkP")

# Check existing indexes
indexes = pc.list_indexes()
print("Available indexes:", indexes)

# Extract the list of index names from the dictionary response
available_indexes = indexes.get('indexes', [])

# Index configuration
index_name = "net-sol-finance"
dimension = 1536
metric = "cosine"
spec = ServerlessSpec(cloud="aws", region="us-east-1")

indexes_data = indexes.get('index', [])

# Create a new index if needed
if index_name not in [idx['name'] for idx in available_indexes]:
    pc.create_index(
        name=index_name,
        dimension=dimension,
        metric=metric,
        spec=spec
    )
    print(f"Index '{index_name}' created successfully.")
else:
    print(f"Index '{index_name}' already exists.")

# Connect to the index
index = pc.Index(index_name)
print(f"Connected to index: {index_name}")

# Upsert example data
embeddings = [[0.1] * dimension for _ in range(10)]  # Replace with your embeddings
vectors = [(str(i), embedding) for i, embedding in enumerate(embeddings)]

index.upsert(vectors)
print("Upsert completed successfully!")

from tavily import TavilyClient

# Initialize the TavilyClient with your API key
tavily_client = TavilyClient(api_key="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")

# Try using the search method
live_data = tavily_client.search(query="latest world news")

# Check available methods in TavilyClient
print(dir(tavily_client))

import requests

def search_live(query):
    base_url = "https://api.tavily.com/search"
    api_key = "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"  # Replace with your actual key

    # Headers for authentication
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Payload
    payload = {"query": query}

    try:
        print("Sending POST request to Tavily API...")
        print(f"Payload: {payload}")

        # Sending request
        response = requests.post(base_url, json=payload, headers=headers)

        # Debugging output
        print("Request URL:", response.request.url)
        print("Request Headers:", response.request.headers)
        print("Status Code:", response.status_code)
        print("Response Headers:", response.headers)
        print("Raw Response:", response.text)

        # Check for errors
        response.raise_for_status()

        # Parse response
        data = response.json()

        print("Parsed Response Data:", data)  # Debugging: Print the structure of the response

        if "results" in data:
            results = data["results"]
         # Check the keys in the first result to understand the structure
            if results:
                print("First Result Keys:", results[0].keys())

            # Format results for output
            formatted_results = "\n".join([f"{i+1}. {result.get('title', 'No title available')}: {result.get('content', 'No content available')}" for i, result in enumerate(results[:5])])
            return f"Top Live Search Results:\n{formatted_results}"
        else:
            return "No relevant live information found for your query."

    except requests.exceptions.RequestException as e:
        return f"An error occurred: {e}"

# Test the function
print(search_live("Latest updates on AI technology"))

from transformers import pipeline

# Load a QA pipeline with a model fine-tuned on SQuAD2.0 (a popular QA dataset)
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Define your financial data (or general context)
financial_data = """
The telephone was invented by Alexander Graham Bell in 1876.
"""

def generate_response(query):
    # Use the QA pipeline to answer the question based on the context
    answer = qa_pipeline(question=query, context=financial_data)
    return answer['answer']

# Example usage
query = "Who invented the telephone?"
response = generate_response(query)
print(response)  # Should return "Alexander Graham Bell"

from transformers import pipeline

# Set up a pretrained model for document question answering (finance-related queries)
# Using a model specifically designed for text-based QA
finance_retriever = pipeline("question-answering", model="deepset/roberta-base-squad2")  # A popular model for question answering

# You can load your financial data here (example using string for simplicity)
financial_data = """
NetSol's revenue for 2024 is $300 million. The company achieved a net profit of $50 million.
Operating expenses were $100 million, and the total assets for the company are $1.5 billion.
"""

def classify_query(query):  # Renamed to classify_query to avoid confusion with route_query
    finance_keywords = ['finance', 'report', 'statement', 'quarterly', 'income', 'profit', 'revenue', 'assets', 'expenses']
    event_keywords = ['news', 'current', 'live', 'update']
    general_keywords = ['who', 'what', 'how', 'why']

    if any(word in query.lower() for word in finance_keywords):
        return 'finance'
    elif any(word in query.lower() for word in event_keywords):
        return 'current_event'  # Changed to 'current_event' to match the elif condition
    else:
        return 'general'

def rag_workflow(query):
    query_type = classify_query(query) # Call the classify_query function

    if query_type == "finance":
        # Use the finance retriever to extract relevant information from the document
        answer = finance_retriever(question=query, context=financial_data)
        return answer['answer']  # Extract the answer part

    elif query_type == "current_event":
        # Fetch live data for current events (use Tavily or another API)
        # Replace with your actual current_event_retriever function
        # answer = current_event_retriever(query)
        answer = "Current event retrieval not implemented yet."
        return answer

    else:  # General question
        # Use Flan-T5 for general questions
        # Replace with your actual general_question_model function
        # answer = general_question_model(query)
        answer = "General question answering not implemented yet."
        return answer

# Example usage
query = "What is the net profit of NetSol for 2024?"
response = rag_workflow(query)
print(response)

def current_event_retriever(query):
    # Example: You can use Tavily, or use a basic API to fetch the current event
    # For now, returning a placeholder response
    return f"Live update for: {query} - Placeholder response. Implement actual live data retrieval."

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Use Flan-T5 (or any other LLM model of your choice)
general_model_name = "google/flan-t5-large"  # Switch to a larger model
general_tokenizer = AutoTokenizer.from_pretrained(general_model_name)
general_model = AutoModelForSeq2SeqLM.from_pretrained(general_model_name)

def general_question_model(query):
    inputs = general_tokenizer(query, return_tensors="pt")
    outputs = general_model.generate(inputs["input_ids"], max_length=256, num_return_sequences=1, no_repeat_ngram_size=2)
    return general_tokenizer.decode(outputs[0], skip_special_tokens=True)

from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer
from nltk.translate.bleu_score import sentence_bleu
# Set up a pretrained model for document question answering (finance-related queries)


# Financial data for testing
financial_data = """
NetSol's revenue for 2024 is $300 million. The company achieved a net profit of $50 million.
Operating expenses were $100 million, and the total assets for the company are $1.5 billion.
"""

# General knowledge context
general_context = """
Historical Facts:
1. The telephone was invented by Alexander Graham Bell in 1876.
2. The light bulb was invented by Thomas Edison.
3. The airplane was invented by the Wright brothers.
"""
# QA pipelines for finance and general questions
finance_retriever = pipeline("question-answering", model="deepset/roberta-base-squad2")
general_retriever = pipeline("question-answering", model="deepset/roberta-base-squad2")

general_model_name = "google/flan-t5-large"  # Switch to a larger model
general_tokenizer = AutoTokenizer.from_pretrained(general_model_name)
general_model = AutoModelForSeq2SeqLM.from_pretrained(general_model_name)


def general_question_model(query, context):
    answer = general_retriever(question=query, context=context)
    return answer['answer']


def current_event_retriever(query):
    return f"Live update for: {query} - Placeholder response. Implement actual live data retrieval."

def classify_query(query):
    finance_keywords = ['finance', 'report', 'statement', 'quarterly', 'income', 'profit', 'revenue', 'assets', 'expenses']
    event_keywords = ['news', 'current', 'live', 'update']
    general_keywords = ['who', 'what', 'how', 'why']

    if any(word in query.lower() for word in finance_keywords):
        return 'finance'
    elif any(word in query.lower() for word in event_keywords):
        return 'current_event'
    else:
        return 'general'

        # Function to evaluate correctness using BLEU score
def evaluate_correctness(reference, generated):
    reference_tokens = reference.split()
    generated_tokens = generated.split()
    score = sentence_bleu([reference_tokens], generated_tokens)
    return score

# Function to evaluate faithfulness (checks if the generated answer is in the context)
def evaluate_faithfulness(query, context, generated_answer):
    if generated_answer in context:
        return "Faithful"
    else:
        return "Not Faithful"

def rag_workflow(query):
    query_type = classify_query(query)
    print("Query type classified as:", query_type)  # Debugging print

    if query_type == "finance":
        print("Using financial context.")
        answer = finance_retriever(question=query, context=financial_data)
        print("Generated Answer (Finance):", answer['answer'])
        return answer['answer']

    else:  # General questions
        print("Using general knowledge context.")
        answer = general_question_model(query, general_context)
        print("Generated Answer (General):", answer)
        return answer

# Example Usage
query = "Who invented the telephone?"
response = rag_workflow(query)
print("Final Response:", response)


import gradio as gr
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer

# Set up a pretrained model for document question answering (finance-related queries)
finance_retriever = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Financial data for testing
financial_data = """
NetSol's revenue for 2024 is $300 million. The company achieved a net profit of $50 million.
Operating expenses were $100 million, and the total assets for the company are $1.5 billion.
"""

# Set up Flan-T5 for general questions
general_model_name = "google/flan-t5-large"
general_tokenizer = AutoTokenizer.from_pretrained(general_model_name)
general_model = AutoModelForSeq2SeqLM.from_pretrained(general_model_name)

def general_question_model(query):
    inputs = general_tokenizer(query, return_tensors="pt")
    outputs = general_model.generate(inputs["input_ids"], max_length=256, num_return_sequences=1, no_repeat_ngram_size=2)
    return general_tokenizer.decode(outputs[0], skip_special_tokens=True)

def current_event_retriever(query):
    return f"Live update for: {query} - Placeholder response. Implement actual live data retrieval."

def classify_query(query):
    finance_keywords = ['finance', 'report', 'statement', 'quarterly', 'income', 'profit', 'revenue', 'assets', 'expenses']
    event_keywords = ['news', 'current', 'live', 'update']
    general_keywords = ['who', 'what', 'how', 'why']

    if any(word in query.lower() for word in finance_keywords):
        return 'finance'
    elif any(word in query.lower() for word in event_keywords):
        return 'current_event'
    else:
        return 'general'

def rag_workflow(query):
    query_type = classify_query(query)

    if query_type == "finance":
        answer = finance_retriever(question=query, context=financial_data)
        return answer['answer']

    elif query_type == "current_event":
        answer = current_event_retriever(query)
        return answer

    else:  # General question
        answer = general_question_model(query)
        return answer

# Create Gradio interface
def chat(query):
    return rag_workflow(query)

# Define the Gradio interface
iface = gr.Interface(fn=chat, inputs="text", outputs="text", title="RAG Chatbot", description="Ask me anything related to NetSol's financial report, current events, or general knowledge.")

# Launch the Gradio app
iface.launch()
